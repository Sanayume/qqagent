"""
LangGraph Agent å›¾å®šä¹‰

ä½¿ç”¨ LangGraph æ„å»º ReAct é£æ ¼çš„ Agentã€‚
æ”¯æŒæ€è€ƒæ¨¡å‹ (Thinking Models) å¦‚ Gemini 2.5 Pro, Claude 3.5 with thinking ç­‰ã€‚
"""

import time
from typing import Literal
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage, BaseMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode

from src.agent.state import AgentState, ChatResponse
from src.core.llm_message import extract_tool_images, extract_send_commands
from src.utils.logger import log


def sanitize_messages_for_api(messages: list[BaseMessage]) -> list[BaseMessage]:
    """æ¸…ç†æ¶ˆæ¯åˆ—è¡¨ï¼Œä½¿å…¶å…¼å®¹ Gemini API çš„ tool calling æ ¼å¼è¦æ±‚

    åªæ¸…ç†å†å²æ¶ˆæ¯ä¸­çš„ tool_calls/ToolMessageï¼Œå½“å‰è½®æ¬¡ä¿æŒåŸæ ·ä»¥ç»´æŒ agent å¾ªç¯ã€‚
    """
    if not messages:
        return messages

    # æ‰¾åˆ°æœ€åä¸€ä¸ª HumanMessage çš„ä½ç½®ï¼ˆå½“å‰è½®æ¬¡å¼€å§‹ï¼‰
    last_human_idx = -1
    for i, msg in enumerate(messages):
        if isinstance(msg, HumanMessage):
            last_human_idx = i

    if last_human_idx == -1:
        return messages

    # åˆ†ç¦»å†å²å’Œå½“å‰è½®æ¬¡
    history = messages[:last_human_idx]
    current_round = messages[last_human_idx:]

    # ==================== åªæ¸…ç†å†å²æ¶ˆæ¯ ====================
    sanitized_history = []
    
    for msg in history:
        if isinstance(msg, ToolMessage):
            # ToolMessage è½¬ä¸º AIMessage çº¯æ–‡æœ¬
            tool_name = getattr(msg, 'name', None) or 'tool'
            content = msg.content if isinstance(msg.content, str) else str(msg.content)
            if len(content) > 200:
                content = content[:200] + "..."
            sanitized_history.append(AIMessage(content=f"[{tool_name}ç»“æœ: {content}]"))

        elif isinstance(msg, AIMessage):
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                # AIMessage with tool_calls è½¬ä¸ºçº¯æ–‡æœ¬
                tool_names = [tc.get('name', 'unknown') for tc in msg.tool_calls]
                text_content = msg.content or f"[è°ƒç”¨äº†å·¥å…·: {', '.join(tool_names)}]"
                sanitized_history.append(AIMessage(content=text_content))
            else:
                sanitized_history.append(msg)
        else:
            sanitized_history.append(msg)

    # å½“å‰è½®æ¬¡ä¿æŒåŸæ ·ï¼ˆåŒ…æ‹¬ tool_calls å’Œ ToolMessageï¼‰
    return sanitized_history + list(current_round)


def create_llm(
    model: str = "gpt-4o-mini",
    api_key: str = "",
    base_url: str = "",
    temperature: float = 0.7,
) -> ChatOpenAI:
    """åˆ›å»º LLM å®ä¾‹

    Args:
        model: æ¨¡å‹åç§°
        api_key: API Key
        base_url: API Base URL (ç”¨äºæœ¬åœ°æ¨¡å‹/ä»£ç†)
        temperature: æ¸©åº¦å‚æ•°
    """
    import httpx

    kwargs = {
        "model": model,
        "temperature": temperature,
        # ç¦ç”¨ç³»ç»Ÿä»£ç†ï¼Œé¿å…æœ¬åœ° API è¯·æ±‚è¢«ä»£ç†æ‹¦æˆª
        "http_client": httpx.Client(trust_env=False),
        "http_async_client": httpx.AsyncClient(trust_env=False),
    }

    if api_key:
        kwargs["api_key"] = api_key
    if base_url:
        kwargs["base_url"] = base_url

    log.info(f"Creating LLM: model={model}, base_url={base_url or 'default'}")

    return ChatOpenAI(**kwargs)


def extract_response_content(ai_message) -> str:
    """ä» AI æ¶ˆæ¯ä¸­æå–å›å¤å†…å®¹
    
    æ”¯æŒæ€è€ƒæ¨¡å‹çš„å¤šç§å“åº”æ ¼å¼:
    1. æ™®é€šæ–‡æœ¬ content
    2. å¸¦ thinking chain çš„å“åº” (content æ˜¯åˆ—è¡¨)
    3. additional_kwargs ä¸­çš„ thinking å­—æ®µ
    """
    # å¦‚æœæ²¡æœ‰ content å±æ€§
    if not hasattr(ai_message, "content"):
        return str(ai_message)
    
    content = ai_message.content
    
    # æƒ…å†µ 1: content æ˜¯å­—ç¬¦ä¸²
    if isinstance(content, str):
        return content
    
    # æƒ…å†µ 2: content æ˜¯åˆ—è¡¨ (æ€è€ƒæ¨¡å‹æ ¼å¼)
    # æ ¼å¼å¯èƒ½æ˜¯: [{"type": "thinking", "thinking": "..."}, {"type": "text", "text": "..."}]
    if isinstance(content, list):
        text_parts = []
        thinking_parts = []
        
        for item in content:
            if isinstance(item, dict):
                item_type = item.get("type", "")
                
                # æå–æ–‡æœ¬å†…å®¹
                if item_type == "text":
                    text_parts.append(item.get("text", ""))
                elif item_type == "thinking":
                    thinking_parts.append(item.get("thinking", ""))
                # OpenAI æ ¼å¼
                elif "text" in item:
                    text_parts.append(item["text"])
            elif isinstance(item, str):
                text_parts.append(item)
        
        # å¦‚æœæœ‰æ€è€ƒè¿‡ç¨‹ï¼Œè®°å½•æ—¥å¿—
        if thinking_parts:
            thinking_summary = thinking_parts[0][:200] + "..." if len(thinking_parts[0]) > 200 else thinking_parts[0]
            log.debug(f"Thinking chain: {thinking_summary}")
        
        # è¿”å›æ–‡æœ¬éƒ¨åˆ†
        if text_parts:
            return "".join(text_parts)
        
        # å¦‚æœåªæœ‰æ€è€ƒæ²¡æœ‰æ–‡æœ¬ï¼Œè¿”å›æ€è€ƒå†…å®¹çš„æ‘˜è¦
        if thinking_parts:
            return thinking_parts[-1][:500] if thinking_parts[-1] else ""
    
    # æƒ…å†µ 3: æ£€æŸ¥ additional_kwargs
    if hasattr(ai_message, "additional_kwargs"):
        kwargs = ai_message.additional_kwargs
        
        # Gemini é£æ ¼
        if "thinking" in kwargs:
            log.debug(f"Thinking from kwargs: {kwargs['thinking'][:100]}...")
        
        # æŸäº›æ¨¡å‹å°†å†…å®¹æ”¾åœ¨è¿™é‡Œ
        if "content" in kwargs and isinstance(kwargs["content"], str):
            return kwargs["content"]
    
    # å…œåº•: è½¬ä¸ºå­—ç¬¦ä¸²
    return str(content) if content else ""


def create_agent_graph(
    llm: ChatOpenAI | None = None,
    tools: list | None = None,
    model: str = "gpt-4o-mini",
    api_key: str = "",
    base_url: str = "",
):
    """åˆ›å»º Agent å›¾

    Args:
        llm: é¢„é…ç½®çš„ LLM å®ä¾‹ (å¯é€‰)
        tools: å·¥å…·åˆ—è¡¨ (é»˜è®¤ä½¿ç”¨å†…ç½®å·¥å…·)
        model: æ¨¡å‹åç§° (å½“ llm ä¸º None æ—¶ä½¿ç”¨)
        api_key: API Key (å½“ llm ä¸º None æ—¶ä½¿ç”¨)
        base_url: API Base URL (å½“ llm ä¸º None æ—¶ä½¿ç”¨)

    Returns:
        ç¼–è¯‘åçš„ LangGraph Agent
    """
    # åˆ›å»º LLM
    if llm is None:
        llm = create_llm(model=model, api_key=api_key, base_url=base_url)

    # ä½¿ç”¨ä¼ å…¥çš„å·¥å…·æˆ–ç©ºåˆ—è¡¨ï¼ˆè°ƒç”¨æ–¹åº”æä¾›å·¥å…·ï¼‰
    if tools is None:
        tools = []

    # ç»‘å®šå·¥å…·åˆ° LLM
    llm_with_tools = llm.bind_tools(tools)

    # åˆ›å»ºå·¥å…·èŠ‚ç‚¹
    tool_node = ToolNode(tools)

    # å¾ªç¯è®¡æ•°å™¨ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    loop_counter = {"count": 0, "start_time": 0}

    # ==================== èŠ‚ç‚¹å‡½æ•° ====================

    def call_model(state: AgentState) -> dict:
        """è°ƒç”¨ LLM ç”Ÿæˆå›å¤"""
        loop_counter["count"] += 1
        loop_num = loop_counter["count"]

        messages = list(state["messages"])

        # å¦‚æœæœ‰ç³»ç»Ÿæç¤ºè¯ï¼Œç¡®ä¿å®ƒåœ¨æœ€å‰é¢
        system_prompt = state.get("system_prompt", "")
        if system_prompt:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç³»ç»Ÿæ¶ˆæ¯
            if not messages or not isinstance(messages[0], SystemMessage):
                messages.insert(0, SystemMessage(content=system_prompt))

        # æ¸…ç†æ¶ˆæ¯æ ¼å¼ï¼Œä¿®å¤ Gemini API çš„ tool_calls/ToolMessage åŒ¹é…è¦æ±‚
        messages = sanitize_messages_for_api(messages)

        # æ—¥å¿—ï¼šæ˜¾ç¤ºå¾ªç¯çŠ¶æ€
        log.info(f"ğŸ¤– Agent å¾ªç¯ #{loop_num} | ğŸ“¨ è°ƒç”¨ LLM ({len(messages)} æ¡æ¶ˆæ¯)")

        # è°ƒç”¨ LLM
        start_time = time.time()
        response = llm_with_tools.invoke(messages)
        elapsed = time.time() - start_time

        # æå–å†…å®¹ç”¨äºæ—¥å¿—
        content_preview = extract_response_content(response)

        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        has_tool_calls = hasattr(response, "tool_calls") and response.tool_calls

        if has_tool_calls:
            tool_names = [tc.get('name', '?') for tc in response.tool_calls]
            log.info(f"ğŸ¤– Agent å¾ªç¯ #{loop_num} | âš¡ LLM å†³å®šè°ƒç”¨å·¥å…·: {', '.join(tool_names)} ({elapsed:.2f}s)")
        elif content_preview:
            preview = content_preview[:80].replace('\n', ' ')
            log.info(f"ğŸ¤– Agent å¾ªç¯ #{loop_num} | ğŸ’¬ LLM å›å¤: {preview}{'...' if len(content_preview) > 80 else ''} ({elapsed:.2f}s)")
        else:
            log.info(f"ğŸ¤– Agent å¾ªç¯ #{loop_num} | ğŸ’¬ LLM å›å¤: [ç©º] ({elapsed:.2f}s)")

        return {"messages": [response]}

    def execute_tools(state: AgentState) -> dict:
        """æ‰§è¡Œå·¥å…·å¹¶è®°å½•æ—¥å¿—"""
        messages = state["messages"]
        last_message = messages[-1]
        loop_num = loop_counter["count"]

        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            for tc in last_message.tool_calls:
                tool_name = tc.get('name', '?')
                tool_args = tc.get('args', {})
                # ç®€åŒ–å‚æ•°æ˜¾ç¤º
                args_preview = str(tool_args)[:100]
                log.info(f"ğŸ”§ å·¥å…·æ‰§è¡Œ | {tool_name}({args_preview}{'...' if len(str(tool_args)) > 100 else ''})")

        # è°ƒç”¨åŸå§‹å·¥å…·èŠ‚ç‚¹
        start_time = time.time()
        result = tool_node.invoke(state)
        elapsed = time.time() - start_time

        # è®°å½•å·¥å…·ç»“æœ
        if "messages" in result:
            for msg in result["messages"]:
                if isinstance(msg, ToolMessage):
                    tool_name = getattr(msg, 'name', '?')
                    content = msg.content if isinstance(msg.content, str) else str(msg.content)
                    preview = content[:80].replace('\n', ' ')
                    log.info(f"ğŸ”§ å·¥å…·ç»“æœ | {tool_name}: {preview}{'...' if len(content) > 80 else ''} ({elapsed:.2f}s)")

        return result

    def should_continue(state: AgentState) -> Literal["tools", "end"]:
        """å†³å®šæ˜¯å¦ç»§ç»­è°ƒç”¨å·¥å…·"""
        messages = state["messages"]
        last_message = messages[-1]
        loop_num = loop_counter["count"]

        # å¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯æœ‰å·¥å…·è°ƒç”¨ï¼Œç»§ç»­æ‰§è¡Œå·¥å…·
        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            return "tools"

        # ç»“æŸå¾ªç¯
        total_time = time.time() - loop_counter["start_time"]
        log.info(f"ğŸ¤– Agent å®Œæˆ | å…± {loop_num} è½®å¾ªç¯, è€—æ—¶ {total_time:.2f}s")
        return "end"

    # ==================== æ„å»ºå›¾ ====================

    # åˆ›å»ºçŠ¶æ€å›¾
    graph = StateGraph(AgentState)

    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("agent", call_model)
    graph.add_node("tools", execute_tools)  # ä½¿ç”¨è‡ªå®šä¹‰çš„å·¥å…·æ‰§è¡Œå‡½æ•°

    # è®¾ç½®å…¥å£ç‚¹
    graph.set_entry_point("agent")

    # æ·»åŠ æ¡ä»¶è¾¹
    graph.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            "end": END,
        }
    )

    # å·¥å…·æ‰§è¡Œåè¿”å› agent
    graph.add_edge("tools", "agent")

    # ç¼–è¯‘å›¾ï¼Œè®¾ç½®é€’å½’é™åˆ¶é˜²æ­¢æ— é™å¾ªç¯
    # recursion_limit é™åˆ¶å›¾çš„æœ€å¤§æ­¥æ•°ï¼ˆæ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œç®—ä¸€æ­¥ï¼‰
    # è®¾ä¸º 20 æ„å‘³ç€å¤§çº¦ 10 è½® agent-tools å¾ªç¯
    compiled = graph.compile()

    # ä¿å­˜å¾ªç¯è®¡æ•°å™¨åˆ°ç¼–è¯‘åçš„å›¾
    compiled._loop_counter = loop_counter

    log.info(f"Agent graph created with {len(tools)} tools: {[t.name for t in tools]}")

    return compiled


class QQAgent:
    """QQ Agent å°è£…ç±»

    æä¾›æ›´ç®€ä¾¿çš„æ¥å£æ¥ä½¿ç”¨ Agentã€‚
    æ”¯æŒæ€è€ƒæ¨¡å‹ (Thinking Models)ã€‚
    """

    def __init__(
        self,
        model: str = "gpt-4o-mini",
        api_key: str = "",
        base_url: str = "",
        tools: list | None = None,
        default_system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚",
        memory_store: "MemoryStore | None" = None,
    ):
        """åˆå§‹åŒ– Agent

        Args:
            model: æ¨¡å‹åç§°
            api_key: API Key
            base_url: API Base URL
            tools: å·¥å…·åˆ—è¡¨
            default_system_prompt: é»˜è®¤ç³»ç»Ÿæç¤ºè¯
            memory_store: ä¼šè¯å†å²å­˜å‚¨ (å¯é€‰ï¼Œä¸ä¼ åˆ™ä½¿ç”¨å†…å­˜å­˜å‚¨)
        """
        self.model = model
        self.api_key = api_key
        self.base_url = base_url
        self.default_system_prompt = default_system_prompt
        self._tools = tools or []

        log.info(f"Initializing QQAgent with model: {model}")

        # åˆ›å»º Agent å›¾
        self.graph = self._create_graph()

        # ä½¿ç”¨å¤–éƒ¨å­˜å‚¨æˆ–å†…éƒ¨å­—å…¸
        self._memory_store = memory_store
        self._internal_sessions: dict[str, list] | None = None if memory_store else {}

    def _create_graph(self):
        """åˆ›å»º LangGraph Agent å›¾
        
        ç”¨äºåˆå§‹åŒ–æˆ–çƒ­é‡è½½æ—¶é‡æ–°åˆ›å»º graphã€‚
        """
        log.debug(f"Creating agent graph with model={self.model}, base_url={self.base_url[:30] if self.base_url else 'default'}...")
        return create_agent_graph(
            model=self.model,
            api_key=self.api_key,
            base_url=self.base_url,
            tools=self._tools,
        )

    def _generate_tools_description(self) -> str:
        """ç”Ÿæˆå·¥å…·æè¿°ï¼Œé™„åŠ åˆ° system prompt"""
        if not self._tools:
            return ""

        lines = ["\n\n## å¯ç”¨å·¥å…·\nä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ï¼š\n"]
        for tool in self._tools:
            # è·å–å·¥å…·åå’Œæè¿°
            name = tool.name
            desc = tool.description.split('\n')[0][:100] if tool.description else "æ— æè¿°"
            lines.append(f"- **{name}**: {desc}")

        lines.append("\nå½“ç”¨æˆ·çš„è¯·æ±‚éœ€è¦ä½¿ç”¨å·¥å…·æ—¶ï¼Œè¯·ä¸»åŠ¨è°ƒç”¨ç›¸åº”çš„å·¥å…·ã€‚")
        return "\n".join(lines)

    def _get_full_system_prompt(self, base_prompt: str) -> str:
        """è·å–å®Œæ•´çš„ system prompt (åŸºç¡€ + å·¥å…·åˆ—è¡¨)"""
        tools_desc = self._generate_tools_description()
        return base_prompt + tools_desc

    def _get_history(self, session_id: str) -> list:
        """è·å–ä¼šè¯å†å²"""
        if self._memory_store:
            return self._memory_store.get_history(session_id)
        return self._internal_sessions.get(session_id, [])

    def _set_history(self, session_id: str, messages: list):
        """è®¾ç½®ä¼šè¯å†å²"""
        if self._memory_store:
            self._memory_store.set_history(session_id, messages)
        else:
            self._internal_sessions[session_id] = messages
    
    async def chat(
        self,
        message: str | HumanMessage,
        session_id: str = "default",
        user_id: int = 0,
        group_id: int | None = None,
        user_name: str = "ç”¨æˆ·",
        system_prompt: str | None = None,
    ) -> ChatResponse:
        """å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯ (å­—ç¬¦ä¸²æˆ– HumanMessageï¼Œæ”¯æŒå¤šæ¨¡æ€)
            session_id: ä¼šè¯ID (ç”¨äºéš”ç¦»å¯¹è¯å†å²)
            user_id: ç”¨æˆ·QQå·
            group_id: ç¾¤å· (ç§èŠä¸º None)
            user_name: ç”¨æˆ·æ˜µç§°
            system_prompt: ç³»ç»Ÿæç¤ºè¯ (å¯é€‰ï¼Œè¦†ç›–é»˜è®¤å€¼)

        Returns:
            ChatResponse å¯¹è±¡ï¼ŒåŒ…å«æ–‡æœ¬å›å¤å’Œå›¾ç‰‡åˆ—è¡¨
        """
        # é‡ç½®å¾ªç¯è®¡æ•°å™¨
        if hasattr(self.graph, '_loop_counter'):
            self.graph._loop_counter["count"] = 0
            self.graph._loop_counter["start_time"] = time.time()

        # æ—¥å¿—ï¼šå¼€å§‹å¤„ç†
        log.info(f"{'â”€' * 50}")
        log.info(f"ğŸš€ Agent å¼€å§‹ | model={self.model} | session={session_id[:20]}")

        # è·å–ä¼šè¯å†å²
        history = self._get_history(session_id)
        log.info(f"ğŸ“š å†å²æ¶ˆæ¯: {len(history)} æ¡")

        # å¤„ç†æ¶ˆæ¯ï¼šæ”¯æŒå­—ç¬¦ä¸²æˆ– HumanMessage
        if isinstance(message, str):
            user_message = HumanMessage(content=message)
            msg_preview = message[:50]
        elif isinstance(message, HumanMessage):
            user_message = message
            # æå–é¢„è§ˆ
            if isinstance(message.content, str):
                msg_preview = message.content[:50]
            else:
                msg_preview = "[å¤šæ¨¡æ€æ¶ˆæ¯]"
        else:
            # å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢
            user_message = HumanMessage(content=str(message))
            msg_preview = str(message)[:50]

        log.info(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {msg_preview}{'...' if len(msg_preview) >= 50 else ''}")

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        history.append(user_message)

        # æ„å»ºçŠ¶æ€
        state: AgentState = {
            "messages": history,
            "session_id": session_id,
            "user_id": user_id,
            "group_id": group_id,
            "user_name": user_name,
            "preset_name": "default",
            "system_prompt": self._get_full_system_prompt(system_prompt or self.default_system_prompt),
            "should_respond": True,
        }

        # è¿è¡Œ Agentï¼Œè®¾ç½®é€’å½’é™åˆ¶é˜²æ­¢æ— é™å¾ªç¯
        # recursion_limit é™åˆ¶å›¾çš„æœ€å¤§æ­¥æ•°ï¼Œè®¾ä¸º 15 çº¦ç­‰äº 7 è½®å·¥å…·è°ƒç”¨
        result = await self.graph.ainvoke(state, {"recursion_limit": 15})

        # è·å–æœ€åçš„ AI å›å¤
        ai_message = result["messages"][-1]

        # ä½¿ç”¨ä¸“é—¨çš„å‡½æ•°æå–å†…å®¹ (æ”¯æŒæ€è€ƒæ¨¡å‹)
        response_text = extract_response_content(ai_message)

        # æå–å·¥å…·è¿”å›çš„å›¾ç‰‡
        tool_images = extract_tool_images(result["messages"])

        # æå– send_message å·¥å…·çš„å‘é€æŒ‡ä»¤
        pending_sends = extract_send_commands(result["messages"])

        # æ›´æ–°ä¼šè¯å†å² (å­˜å‚¨æ—¶å°†å¤šæ¨¡æ€æ¶ˆæ¯è½¬ä¸ºçº¯æ–‡æœ¬æè¿°)
        final_messages = list(result["messages"])
        self._set_history(session_id, final_messages)

        log.info(f"{'â”€' * 50}")

        return ChatResponse(text=response_text, images=tool_images, pending_sends=pending_sends)

    def clear_session(self, session_id: str):
        """æ¸…é™¤ä¼šè¯å†å²"""
        if self._memory_store:
            self._memory_store.clear(session_id)
        elif session_id in self._internal_sessions:
            del self._internal_sessions[session_id]
            log.info(f"Session {session_id} cleared")

    def get_session_ids(self) -> list[str]:
        """è·å–æ‰€æœ‰æ´»è·ƒçš„ä¼šè¯ID"""
        if self._memory_store:
            return self._memory_store.get_all_session_ids()
        return list(self._internal_sessions.keys())

